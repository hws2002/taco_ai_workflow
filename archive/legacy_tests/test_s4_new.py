"""
S4 ë‹¨ê³„ í…ŒìŠ¤íŠ¸: ìœ ì‚¬ë„ ê³„ì‚° ë° ê²°ê³¼ ë¶„ì„
(ìƒˆë¡œìš´ ì›Œí¬í”Œë¡œìš°: AI ë‹µë³€ ê¸°ë°˜ ë¬¸ì„œ ë¶„ë¥˜)
"""

import sys
from pathlib import Path
import time
import json
import pickle
import numpy as np

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from analyze.embedding_processor import EmbeddingProcessor
from analyze.incremental_cache import IncrementalCache


def test_s4_new(sample_size=50, use_cache=True, similarity_threshold=0.7):
    """
    S4 í…ŒìŠ¤íŠ¸: ìœ ì‚¬ë„ ê³„ì‚° ë° ê²°ê³¼ ë¶„ì„

    ìƒˆ ì›Œí¬í”Œë¡œìš°:
    1. ë¬¸ì„œ ì„ë² ë”© ë¡œë“œ
    2. ìœ ì‚¬ë„ ê³„ì‚° (ì¦ë¶„ ìºì‹±)
    3. ìœ ì‚¬í•œ ë¬¸ì„œ ìŒ ë¶„ì„
    4. ê²°ê³¼ ì €ì¥
    """
    start_time = time.time()
    times = {}

    print("=" * 80)
    print("S4 ë‹¨ê³„ í…ŒìŠ¤íŠ¸: ìœ ì‚¬ë„ ê³„ì‚° ë° ê²°ê³¼ ë¶„ì„")
    print("(ìƒˆë¡œìš´ ì›Œí¬í”Œë¡œìš°: AI ë‹µë³€ ê¸°ë°˜)")
    print("=" * 80)

    output_dir = project_root / "test" / "output"
    output_dir.mkdir(exist_ok=True)

    # ============================================================
    # 1. ë¬¸ì„œ ì„ë² ë”© ë¡œë“œ
    # ============================================================
    print("\n[1] ë¬¸ì„œ ì„ë² ë”© ë¡œë“œ")
    print("-" * 80)

    load_start = time.time()

    # S3 ê²°ê³¼ íŒŒì¼ ë¡œë“œ
    doc_embeddings_file = output_dir / "s3_document_embeddings.pkl"

    if not doc_embeddings_file.exists():
        print(f"âŒ S3 ê²°ê³¼ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {doc_embeddings_file}")
        print("ë¨¼ì € test_s3_new.pyë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
        return None

    with open(doc_embeddings_file, 'rb') as f:
        document_embeddings = pickle.load(f)

    print(f"âœ“ {len(document_embeddings)}ê°œì˜ ë¬¸ì„œ ì„ë² ë”© ë¡œë“œ ì™„ë£Œ")

    times['loading'] = time.time() - load_start
    print(f"â±ï¸  ì†Œìš” ì‹œê°„: {times['loading']:.2f}ì´ˆ")

    # ============================================================
    # 2. ìœ ì‚¬ë„ ê³„ì‚° (ì¦ë¶„ ìºì‹±)
    # ============================================================
    print("\n" + "=" * 80)
    print("[2] ìœ ì‚¬ë„ ê³„ì‚° (ì¦ë¶„ ìºì‹± ì‚¬ìš©)")
    print("-" * 80)

    similarity_start = time.time()

    processor = EmbeddingProcessor(
        min_topic_size=3,
        nr_topics=None,
        language="multilingual",
        verbose=False
    )

    if use_cache:
        # ì¦ë¶„ ìºì‹œ ì‚¬ìš©
        cache = IncrementalCache(cache_dir="cache")
        similarities_cache = cache.load_similarities_cache()

        # ëª¨ë“  ëŒ€í™” ID
        all_conv_ids = list(document_embeddings.keys())

        # ìºì‹œì— ì—†ëŠ” ìŒ ì°¾ê¸°
        missing_pairs = cache.get_missing_similarities(all_conv_ids, similarities_cache)

        total_pairs = len(all_conv_ids) * (len(all_conv_ids) - 1) // 2

        print(f"ì „ì²´ ìŒ: {total_pairs}ê°œ")
        print(f"ìºì‹œì— ìˆìŒ: {total_pairs - len(missing_pairs)}ê°œ")
        print(f"ìƒˆë¡œ ê³„ì‚° í•„ìš”: {len(missing_pairs)}ê°œ")

        if missing_pairs:
            print(f"\n{len(missing_pairs)}ê°œì˜ ìƒˆë¡œìš´ ìœ ì‚¬ë„ ê³„ì‚° ì¤‘...")

            # ì—†ëŠ” ìŒë§Œ ê³„ì‚°
            new_similarities = {}
            for conv_id_1, conv_id_2 in missing_pairs:
                doc1 = document_embeddings[conv_id_1]
                doc2 = document_embeddings[conv_id_2]
                sim = processor.compute_document_similarity(doc1, doc2)
                new_similarities[(conv_id_1, conv_id_2)] = sim

            # ìºì‹œ ì—…ë°ì´íŠ¸
            similarities_cache = cache.update_similarities_cache(similarities_cache, new_similarities)
            cache.save_similarities_cache(similarities_cache)

            print(f"âœ“ {len(new_similarities)}ê°œì˜ ìƒˆë¡œìš´ ìœ ì‚¬ë„ ì¶”ê°€")
        else:
            print("âœ“ ëª¨ë“  ìœ ì‚¬ë„ê°€ ìºì‹œì— ìˆìŒ (ê³„ì‚° ê±´ë„ˆëœ€)")

        # ìµœì¢… ê²°ê³¼: í˜„ì¬ í•„ìš”í•œ ìŒë“¤ì˜ ìœ ì‚¬ë„ë§Œ ì¶”ì¶œ
        similarities = {}
        for i, conv_id_1 in enumerate(all_conv_ids):
            for conv_id_2 in all_conv_ids[i + 1:]:
                pair = tuple(sorted([conv_id_1, conv_id_2]))
                if pair in similarities_cache:
                    similarities[(conv_id_1, conv_id_2)] = similarities_cache[pair]

    else:
        # ìºì‹œ ì‚¬ìš© ì•ˆ í•¨
        print("ìºì‹œ ì‚¬ìš© ì•ˆ í•¨ - ì „ì²´ ê³„ì‚°")
        similarities = processor.compute_all_document_similarities(document_embeddings)

    times['similarity'] = time.time() - similarity_start
    print(f"â±ï¸  ì†Œìš” ì‹œê°„: {times['similarity']:.2f}ì´ˆ")

    # ìœ ì‚¬ë„ í†µê³„
    sim_values = list(similarities.values())
    print(f"\nìœ ì‚¬ë„ í†µê³„:")
    print(f"  í‰ê· : {np.mean(sim_values):.4f}")
    print(f"  ì¤‘ì•™ê°’: {np.median(sim_values):.4f}")
    print(f"  í‘œì¤€í¸ì°¨: {np.std(sim_values):.4f}")
    print(f"  ìµœëŒ€: {np.max(sim_values):.4f}")
    print(f"  ìµœì†Œ: {np.min(sim_values):.4f}")

    # ============================================================
    # 3. ìœ ì‚¬í•œ ë¬¸ì„œ ìŒ ë¶„ì„
    # ============================================================
    print("\n" + "=" * 80)
    print("[3] ìœ ì‚¬í•œ ë¬¸ì„œ ìŒ ë¶„ì„")
    print("-" * 80)

    analysis_start = time.time()

    # ìœ ì‚¬ë„ ì„ê³„ê°’ ì´ìƒì¸ ìŒ í•„í„°ë§
    high_similarity_pairs = [
        (pair, sim)
        for pair, sim in similarities.items()
        if sim >= similarity_threshold
    ]

    print(f"ìœ ì‚¬ë„ {similarity_threshold} ì´ìƒì¸ ìŒ: {len(high_similarity_pairs)}ê°œ")

    # ê°€ì¥ ìœ ì‚¬í•œ ìŒ TOP 10
    sorted_sims = sorted(similarities.items(), key=lambda x: x[1], reverse=True)

    print(f"\nê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œ ìŒ TOP 10:")
    for i, ((conv_id_1, conv_id_2), sim) in enumerate(sorted_sims[:10], 1):
        doc1 = document_embeddings[conv_id_1]
        doc2 = document_embeddings[conv_id_2]
        print(f"\n  {i}. ìœ ì‚¬ë„ {sim:.4f}")
        print(f"     [{conv_id_1}] {doc1.conversation_title}")
        print(f"     [{conv_id_2}] {doc2.conversation_title}")

    times['analysis'] = time.time() - analysis_start

    # ============================================================
    # 4. ê²°ê³¼ ì €ì¥
    # ============================================================
    print("\n" + "=" * 80)
    print("[4] ê²°ê³¼ ì €ì¥")
    print("-" * 80)

    save_start = time.time()

    # ìœ ì‚¬ë„ ì €ì¥ (pickle)
    similarities_file = output_dir / "s4_similarities.pkl"
    with open(similarities_file, 'wb') as f:
        pickle.dump(similarities, f)

    print(f"âœ“ ìœ ì‚¬ë„ ì €ì¥: {similarities_file}")
    print(f"  - ì´ {len(similarities)}ê°œ ìŒ")
    print(f"  - íŒŒì¼ í¬ê¸°: {similarities_file.stat().st_size / 1024:.2f} KB")

    # ê³ ìœ ì‚¬ë„ ìŒ ì €ì¥ (JSON - ê°€ë…ì„±)
    high_sim_data = []
    for (conv_id_1, conv_id_2), sim in sorted_sims[:50]:  # TOP 50
        doc1 = document_embeddings[conv_id_1]
        doc2 = document_embeddings[conv_id_2]
        high_sim_data.append({
            'conversation_1': {
                'id': conv_id_1,
                'title': doc1.conversation_title,
                'response_count': doc1.response_count,
                'topic_count': len(doc1.topic_embeddings)
            },
            'conversation_2': {
                'id': conv_id_2,
                'title': doc2.conversation_title,
                'response_count': doc2.response_count,
                'topic_count': len(doc2.topic_embeddings)
            },
            'similarity': float(sim)
        })

    high_sim_file = output_dir / "s4_high_similarities.json"
    with open(high_sim_file, 'w', encoding='utf-8') as f:
        json.dump(high_sim_data, f, ensure_ascii=False, indent=2)

    print(f"âœ“ ê³ ìœ ì‚¬ë„ ìŒ ì €ì¥: {high_sim_file}")

    # í†µê³„ ì •ë³´ ì €ì¥
    times['saving'] = time.time() - save_start
    times['total'] = time.time() - start_time

    stats = {
        "total_documents": len(document_embeddings),
        "total_similarity_pairs": len(similarities),
        "high_similarity_pairs": len(high_similarity_pairs),
        "similarity_threshold": similarity_threshold,
        "similarity_statistics": {
            "mean": float(np.mean(sim_values)),
            "median": float(np.median(sim_values)),
            "std": float(np.std(sim_values)),
            "min": float(np.min(sim_values)),
            "max": float(np.max(sim_values))
        },
        "elapsed_time": {
            "loading_seconds": round(times['loading'], 2),
            "similarity_seconds": round(times['similarity'], 2),
            "analysis_seconds": round(times['analysis'], 2),
            "saving_seconds": round(times['saving'], 2),
            "total_seconds": round(times['total'], 2)
        }
    }

    stats_file = output_dir / "s4_stats.json"
    with open(stats_file, 'w', encoding='utf-8') as f:
        json.dump(stats, f, ensure_ascii=False, indent=2)

    print(f"âœ“ í†µê³„ ì €ì¥: {stats_file}")

    print("\n" + "=" * 80)
    print("S4 ë‹¨ê³„ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print("=" * 80)
    print(f"\nâ±ï¸  ì†Œìš” ì‹œê°„:")
    print(f"  - ë¬¸ì„œ ì„ë² ë”© ë¡œë“œ: {times['loading']:.2f}ì´ˆ")
    print(f"  - ìœ ì‚¬ë„ ê³„ì‚°: {times['similarity']:.2f}ì´ˆ")
    print(f"  - ê²°ê³¼ ë¶„ì„: {times['analysis']:.2f}ì´ˆ")
    print(f"  - ê²°ê³¼ ì €ì¥: {times['saving']:.2f}ì´ˆ")
    print(f"  - ì „ì²´: {times['total']:.2f}ì´ˆ")

    print(f"\nğŸ“Š ê²°ê³¼ ìš”ì•½:")
    print(f"  - ë¬¸ì„œ ìˆ˜: {len(document_embeddings)}ê°œ")
    print(f"  - ìœ ì‚¬ë„ ìŒ ìˆ˜: {len(similarities)}ê°œ")
    print(f"  - ê³ ìœ ì‚¬ë„ ìŒ (â‰¥{similarity_threshold}): {len(high_similarity_pairs)}ê°œ")
    print(f"  - í‰ê·  ìœ ì‚¬ë„: {np.mean(sim_values):.4f}")

    return similarities


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="S4 í…ŒìŠ¤íŠ¸: ìœ ì‚¬ë„ ê³„ì‚°")
    parser.add_argument("--sample-size", type=int, default=50, help="ìƒ˜í”Œ ëŒ€í™” ê°œìˆ˜")
    parser.add_argument("--threshold", type=float, default=0.7, help="ìœ ì‚¬ë„ ì„ê³„ê°’")
    parser.add_argument("--no-cache", action="store_true", help="ìºì‹œ ì‚¬ìš© ì•ˆ í•¨")
    args = parser.parse_args()

    try:
        similarities = test_s4_new(
            sample_size=args.sample_size,
            use_cache=not args.no_cache,
            similarity_threshold=args.threshold
        )

        if similarities:
            print(f"\nâœ… S4 í…ŒìŠ¤íŠ¸ ì„±ê³µ!")

    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
