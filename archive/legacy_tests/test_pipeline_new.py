"""
ì „ì²´ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ (S1 â†’ S2 â†’ S3 â†’ S4)
(ìƒˆë¡œìš´ ì›Œí¬í”Œë¡œìš°: AI ë‹µë³€ ê¸°ë°˜ ë¬¸ì„œ ë¶„ë¥˜)
"""

import sys
from pathlib import Path
import time
import argparse

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from test_s1_new import test_s1_new
from test_s2_new import test_s2_new
from test_s3_new import test_s3_new
from test_s4_new import test_s4_new
from test_s5_llm_clustering import test_s5_llm_clustering


def test_full_pipeline(
    sample_size=50,
    use_cache=True,
    n_clusters=10,
    similarity_threshold=0.7,
    llm_provider="openai",
    llm_model="gpt-4",
    llm_n_clusters=None
):
    """
    ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰

    S1: AI ë‹µë³€ ì¶”ì¶œ
    S2: ì„ë² ë”© ìƒì„±
    S3: BERTopic í´ëŸ¬ìŠ¤í„°ë§ ë° ë¬¸ì„œë³„ í’€ë§
    S4: ìœ ì‚¬ë„ ê³„ì‚°
    S5: LLM ê¸°ë°˜ ëŒ€ë¶„ë¥˜ í´ëŸ¬ìŠ¤í„°ë§

    Args:
        sample_size: ìƒ˜í”Œ ëŒ€í™” ê°œìˆ˜
        use_cache: ì¦ë¶„ ìºì‹± ì‚¬ìš© ì—¬ë¶€
        n_clusters: K-Means í´ëŸ¬ìŠ¤í„° ê°œìˆ˜
        similarity_threshold: ìœ ì‚¬ë„ ì„ê³„ê°’
        llm_provider: LLM ì œê³µì (openai/anthropic)
        llm_model: LLM ëª¨ë¸ëª…
        llm_n_clusters: LLM í´ëŸ¬ìŠ¤í„° ê°œìˆ˜ (Noneì´ë©´ ìë™)
    """
    total_start = time.time()

    print("=" * 80)
    print("ì „ì²´ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸")
    print("(ìƒˆë¡œìš´ ì›Œí¬í”Œë¡œìš°: AI ë‹µë³€ ê¸°ë°˜ ë¬¸ì„œ ë¶„ë¥˜)")
    print("=" * 80)
    print(f"\nì„¤ì •:")
    print(f"  - ìƒ˜í”Œ í¬ê¸°: {sample_size}ê°œ ëŒ€í™”")
    print(f"  - ì¦ë¶„ ìºì‹±: {'ì‚¬ìš©' if use_cache else 'ì‚¬ìš© ì•ˆ í•¨'}")
    print(f"  - BERTopic í´ëŸ¬ìŠ¤í„° ê°œìˆ˜: {n_clusters}ê°œ")
    print(f"  - ìœ ì‚¬ë„ ì„ê³„ê°’: {similarity_threshold}")
    print(f"  - LLM ì œê³µì: {llm_provider}")
    print(f"  - LLM ëª¨ë¸: {llm_model}")
    print(f"  - LLM í´ëŸ¬ìŠ¤í„°: {llm_n_clusters if llm_n_clusters else 'ìë™ ê²°ì •'}")
    print("=" * 80)

    # ============================================================
    # S1: AI ë‹µë³€ ì¶”ì¶œ
    # ============================================================
    print("\n\n" + "ğŸ”µ" * 40)
    print("S1: AI ë‹µë³€ ì¶”ì¶œ")
    print("ğŸ”µ" * 40)

    s1_start = time.time()
    ai_responses = test_s1_new(sample_size=sample_size, use_cache=use_cache)
    s1_time = time.time() - s1_start

    if not ai_responses:
        print("\nâŒ S1 ì‹¤íŒ¨")
        return

    print(f"\nâœ… S1 ì™„ë£Œ ({s1_time:.1f}ì´ˆ)")

    # ============================================================
    # S2: ì„ë² ë”© ìƒì„±
    # ============================================================
    print("\n\n" + "ğŸŸ¢" * 40)
    print("S2: ì„ë² ë”© ìƒì„±")
    print("ğŸŸ¢" * 40)

    s2_start = time.time()
    response_embeddings = test_s2_new(sample_size=sample_size, use_cache=use_cache)
    s2_time = time.time() - s2_start

    if not response_embeddings:
        print("\nâŒ S2 ì‹¤íŒ¨")
        return

    print(f"\nâœ… S2 ì™„ë£Œ ({s2_time:.1f}ì´ˆ)")

    # ============================================================
    # S3: BERTopic í´ëŸ¬ìŠ¤í„°ë§ ë° ë¬¸ì„œë³„ í’€ë§
    # ============================================================
    print("\n\n" + "ğŸŸ¡" * 40)
    print("S3: BERTopic í´ëŸ¬ìŠ¤í„°ë§ ë° ë¬¸ì„œë³„ í’€ë§")
    print("ğŸŸ¡" * 40)

    s3_start = time.time()
    document_embeddings = test_s3_new(
        sample_size=sample_size,
        use_cache=use_cache,
        n_clusters=n_clusters
    )
    s3_time = time.time() - s3_start

    if not document_embeddings:
        print("\nâŒ S3 ì‹¤íŒ¨")
        return

    print(f"\nâœ… S3 ì™„ë£Œ ({s3_time:.1f}ì´ˆ)")

    # ============================================================
    # S4: ìœ ì‚¬ë„ ê³„ì‚°
    # ============================================================
    print("\n\n" + "ğŸ”´" * 40)
    print("S4: ìœ ì‚¬ë„ ê³„ì‚°")
    print("ğŸ”´" * 40)

    s4_start = time.time()
    similarities = test_s4_new(
        sample_size=sample_size,
        use_cache=use_cache,
        similarity_threshold=similarity_threshold
    )
    s4_time = time.time() - s4_start

    if not similarities:
        print("\nâŒ S4 ì‹¤íŒ¨")
        return

    print(f"\nâœ… S4 ì™„ë£Œ ({s4_time:.1f}ì´ˆ)")

    # ============================================================
    # S5: LLM ê¸°ë°˜ ëŒ€ë¶„ë¥˜ í´ëŸ¬ìŠ¤í„°ë§
    # ============================================================
    print("\n\n" + "ğŸŸ£" * 40)
    print("S5: LLM ê¸°ë°˜ ëŒ€ë¶„ë¥˜ í´ëŸ¬ìŠ¤í„°ë§")
    print("ğŸŸ£" * 40)

    s5_start = time.time()
    llm_clusters = test_s5_llm_clustering(
        sample_size=sample_size,
        use_cache=use_cache,
        provider=llm_provider,
        model=llm_model,
        n_clusters=llm_n_clusters
    )
    s5_time = time.time() - s5_start

    if not llm_clusters:
        print("\nâŒ S5 ì‹¤íŒ¨")
        return

    print(f"\nâœ… S5 ì™„ë£Œ ({s5_time:.1f}ì´ˆ)")

    # ============================================================
    # ì „ì²´ ìš”ì•½
    # ============================================================
    total_time = time.time() - total_start

    print("\n\n" + "=" * 80)
    print("ì „ì²´ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")
    print("=" * 80)

    print(f"\nâ±ï¸  ë‹¨ê³„ë³„ ì†Œìš” ì‹œê°„:")
    print(f"  S1 (AI ë‹µë³€ ì¶”ì¶œ)      : {s1_time:6.1f}ì´ˆ  ({s1_time/total_time*100:5.1f}%)")
    print(f"  S2 (ì„ë² ë”© ìƒì„±)       : {s2_time:6.1f}ì´ˆ  ({s2_time/total_time*100:5.1f}%)")
    print(f"  S3 (í´ëŸ¬ìŠ¤í„°ë§/í’€ë§)   : {s3_time:6.1f}ì´ˆ  ({s3_time/total_time*100:5.1f}%)")
    print(f"  S4 (ìœ ì‚¬ë„ ê³„ì‚°)       : {s4_time:6.1f}ì´ˆ  ({s4_time/total_time*100:5.1f}%)")
    print(f"  S5 (LLM ëŒ€ë¶„ë¥˜)        : {s5_time:6.1f}ì´ˆ  ({s5_time/total_time*100:5.1f}%)")
    print(f"  " + "-" * 60)
    print(f"  ì „ì²´                   : {total_time:6.1f}ì´ˆ  (100.0%)")

    print(f"\nğŸ“Š ìµœì¢… ê²°ê³¼:")
    print(f"  - AI ë‹µë³€: {len(ai_responses)}ê°œ")
    print(f"  - ì„ë² ë”©: {len(response_embeddings)}ê°œ")
    print(f"  - ë¬¸ì„œ: {len(document_embeddings)}ê°œ")
    print(f"  - ìœ ì‚¬ë„ ìŒ: {len(similarities)}ê°œ")
    print(f"  - LLM í´ëŸ¬ìŠ¤í„°: {len(llm_clusters.get('cluster_definitions', {}))}ê°œ")

    print(f"\nğŸ“ ì¶œë ¥ íŒŒì¼:")
    output_dir = project_root / "test" / "output"
    print(f"  - {output_dir / 's1_ai_responses.json'}")
    print(f"  - {output_dir / 's2_embeddings.pkl'}")
    print(f"  - {output_dir / 's3_document_embeddings.pkl'}")
    print(f"  - {output_dir / 's4_similarities.pkl'}")
    print(f"  - {output_dir / 's4_high_similarities.json'}")
    print(f"  - {output_dir / 's5_keywords.json'}")
    print(f"  - {output_dir / 's5_llm_clustering_result.json'}")

    print("\n" + "=" * 80)


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="ì „ì²´ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸")

    parser.add_argument(
        "--sample-size",
        type=int,
        default=50,
        help="ìƒ˜í”Œ ëŒ€í™” ê°œìˆ˜ (ê¸°ë³¸: 50)"
    )

    parser.add_argument(
        "--no-cache",
        action="store_true",
        help="ì¦ë¶„ ìºì‹± ì‚¬ìš© ì•ˆ í•¨"
    )

    parser.add_argument(
        "--n-clusters",
        type=int,
        default=10,
        help="K-Means í´ëŸ¬ìŠ¤í„° ê°œìˆ˜ (ê¸°ë³¸: 10)"
    )

    parser.add_argument(
        "--threshold",
        type=float,
        default=0.7,
        help="ìœ ì‚¬ë„ ì„ê³„ê°’ (ê¸°ë³¸: 0.7)"
    )

    parser.add_argument(
        "--llm-provider",
        type=str,
        default="openai",
        choices=["openai", "anthropic"],
        help="LLM ì œê³µì (ê¸°ë³¸: openai)"
    )

    parser.add_argument(
        "--llm-model",
        type=str,
        default="gpt-4",
        help="LLM ëª¨ë¸ (ê¸°ë³¸: gpt-4)"
    )

    parser.add_argument(
        "--llm-n-clusters",
        type=int,
        default=None,
        help="LLM í´ëŸ¬ìŠ¤í„° ê°œìˆ˜ (ë¯¸ì§€ì •ì‹œ ìë™ ê²°ì •)"
    )

    args = parser.parse_args()

    try:
        test_full_pipeline(
            sample_size=args.sample_size,
            use_cache=not args.no_cache,
            n_clusters=args.n_clusters,
            similarity_threshold=args.threshold,
            llm_provider=args.llm_provider,
            llm_model=args.llm_model,
            llm_n_clusters=args.llm_n_clusters
        )

        print("\nâœ… ì „ì²´ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ì„±ê³µ!")

    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
