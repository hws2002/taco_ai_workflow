"""
ì „ì²´ ì„ë² ë”© ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸
AI ë‹µë³€ ì¶”ì¶œ -> ì„ë² ë”© ìƒì„± -> BERTopic í´ëŸ¬ìŠ¤í„°ë§ -> ë¬¸ì„œë³„ í’€ë§ -> ìœ ì‚¬ë„ ê³„ì‚°
"""

import sys
import argparse
import time
from pathlib import Path
from datetime import timedelta

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ sys.pathì— ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from analyze.loader import ConversationLoader
from analyze.parser import NoteParser
from analyze.semantic_analyzer import SemanticAnalyzer
from analyze.embedding_processor import EmbeddingProcessor
from analyze.cache_manager import CacheManager
from analyze.incremental_cache import IncrementalCache


def format_time(seconds: float) -> str:
    """ì‹œê°„ì„ ì½ê¸° ì‰¬ìš´ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
    if seconds < 1:
        return f"{seconds * 1000:.0f}ms"
    elif seconds < 60:
        return f"{seconds:.1f}ì´ˆ"
    else:
        minutes = int(seconds // 60)
        secs = seconds % 60
        return f"{minutes}ë¶„ {secs:.1f}ì´ˆ"


def main(use_cache: bool = False, clear_cache: bool = False, sample_size: int = 50, use_incremental: bool = True):
    # ì „ì²´ ì‹œì‘ ì‹œê°„
    total_start = time.time()

    # ê° ìŠ¤í…ë³„ ì‹œê°„ ê¸°ë¡
    step_times = {}

    print("=" * 80)
    print("AI ë‹µë³€ ê¸°ë°˜ ë¬¸ì„œ ì„ë² ë”© ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸")
    if use_incremental:
        print("(ì¦ë¶„ ìºì‹± ëª¨ë“œ)")
    print("=" * 80)

    # ìºì‹œ ê´€ë¦¬ì ì´ˆê¸°í™”
    if use_incremental:
        cache_manager = IncrementalCache(cache_dir="cache")
    else:
        cache_manager = CacheManager(cache_dir="cache")

    # ìºì‹œ ì´ˆê¸°í™” ìš”ì²­ ì‹œ
    if clear_cache:
        cache_manager.clear_cache()
        print("âœ“ ìºì‹œ ì´ˆê¸°í™” ì™„ë£Œ\n")

    # ìºì‹œ ì •ë³´ ì¶œë ¥
    if use_cache and use_incremental:
        cache_manager.print_cache_info()

    # ============================================================
    # STEP 1: ë°ì´í„° ë¡œë”©
    # ============================================================
    print("\n[STEP 1] ë°ì´í„° ë¡œë”©")
    print("-" * 80)

    step_start = time.time()

    loader = ConversationLoader()

    # ìƒ˜í”Œ ë°ì´í„°ë¡œ í…ŒìŠ¤íŠ¸ (ì „ì²´ ë°ì´í„°ëŠ” ë„ˆë¬´ í´ ìˆ˜ ìˆìŒ)
    print(f"ìƒ˜í”Œ {sample_size}ê°œ ëŒ€í™” ë¡œë”© ì¤‘...")
    conversations = loader.load_sample(n=sample_size)

    step_times['ë°ì´í„° ë¡œë”©'] = time.time() - step_start
    print(f"â±ï¸  ì†Œìš” ì‹œê°„: {format_time(step_times['ë°ì´í„° ë¡œë”©'])}")

    # ============================================================
    # STEP 2: AI ë‹µë³€ë§Œ ì¶”ì¶œ
    # ============================================================
    print("\n[STEP 2] AI ë‹µë³€ë§Œ ì¶”ì¶œ")
    print("-" * 80)

    step_start = time.time()

    parser = NoteParser(min_content_length=20)
    ai_responses = parser.parse_ai_responses(conversations)

    print(f"âœ“ ì´ {len(ai_responses)}ê°œì˜ AI ë‹µë³€ ì¶”ì¶œ ì™„ë£Œ")

    step_times['AI ë‹µë³€ ì¶”ì¶œ'] = time.time() - step_start
    print(f"â±ï¸  ì†Œìš” ì‹œê°„: {format_time(step_times['AI ë‹µë³€ ì¶”ì¶œ'])}")

    # ìƒ˜í”Œ ì¶œë ¥
    if ai_responses:
        print(f"\nìƒ˜í”Œ AI ë‹µë³€:")
        sample_resp = ai_responses[0]
        print(f"  ID: {sample_resp.response_id}")
        print(f"  ëŒ€í™” ì œëª©: {sample_resp.conversation_title}")
        print(f"  ë‚´ìš© (ì• 100ì): {sample_resp.content[:100]}...")

    # ============================================================
    # STEP 3: ì„ë² ë”© ìƒì„±
    # ============================================================
    print("\n[STEP 3] ì„ë² ë”© ë²¡í„° ìƒì„±")
    print("-" * 80)

    step_start = time.time()

    if use_cache and use_incremental:
        # ì¦ë¶„ ìºì‹±: ID ê¸°ë°˜ìœ¼ë¡œ í•„ìš”í•œ ê²ƒë§Œ ê³„ì‚°
        embeddings_cache = cache_manager.load_embeddings_cache()

        # ëª¨ë“  ì‘ë‹µ ID
        all_response_ids = [resp.response_id for resp in ai_responses]

        # ìºì‹œì— ì—†ëŠ” ì‘ë‹µ ID ì°¾ê¸°
        missing_ids = cache_manager.get_missing_embeddings(all_response_ids, embeddings_cache)

        print(f"ì „ì²´ ì‘ë‹µ: {len(all_response_ids)}ê°œ")
        print(f"ìºì‹œì— ìˆìŒ: {len(all_response_ids) - len(missing_ids)}ê°œ")
        print(f"ìƒˆë¡œ ê³„ì‚° í•„ìš”: {len(missing_ids)}ê°œ")

        if missing_ids:
            # ì—†ëŠ” ê²ƒë§Œ ê³„ì‚°
            print(f"\n{len(missing_ids)}ê°œì˜ ìƒˆë¡œìš´ ì„ë² ë”© ê³„ì‚° ì¤‘...")

            analyzer = SemanticAnalyzer(
                model_name="BAAI/bge-m3",
                use_keybert=True
            )

            # missing_idsì— í•´ë‹¹í•˜ëŠ” ì‘ë‹µë§Œ í•„í„°ë§
            missing_responses = [resp for resp in ai_responses if resp.response_id in missing_ids]
            new_embeddings = analyzer.analyze_ai_responses(missing_responses)

            # ìºì‹œ ì—…ë°ì´íŠ¸
            embeddings_cache = cache_manager.update_embeddings_cache(embeddings_cache, new_embeddings)
            cache_manager.save_embeddings_cache(embeddings_cache)

            print(f"âœ“ {len(new_embeddings)}ê°œì˜ ìƒˆë¡œìš´ ì„ë² ë”© ì¶”ê°€")
        else:
            print("âœ“ ëª¨ë“  ì„ë² ë”©ì´ ìºì‹œì— ìˆìŒ (ê³„ì‚° ê±´ë„ˆëœ€)")

        # ìµœì¢… ê²°ê³¼: í˜„ì¬ í•„ìš”í•œ ì‘ë‹µë“¤ì˜ ì„ë² ë”©ë§Œ ì¶”ì¶œ
        response_embeddings = {rid: embeddings_cache[rid] for rid in all_response_ids}

    elif use_cache and not use_incremental:
        # ê¸°ì¡´ ë°©ì‹: ì „ì²´ ì €ì¥/ë¡œë“œ
        response_embeddings = cache_manager.load_embeddings()

        if response_embeddings is None:
            analyzer = SemanticAnalyzer(
                model_name="BAAI/bge-m3",
                use_keybert=True
            )

            response_embeddings = analyzer.analyze_ai_responses(ai_responses)
            print(f"âœ“ ì´ {len(response_embeddings)}ê°œì˜ ì„ë² ë”© ìƒì„± ì™„ë£Œ")

            cache_manager.save_embeddings(response_embeddings)
        else:
            print("âœ“ ìºì‹œì—ì„œ ì„ë² ë”© ë¡œë“œ ì™„ë£Œ (ê³„ì‚° ê±´ë„ˆëœ€)")

    else:
        # ìºì‹œ ì‚¬ìš© ì•ˆ í•¨
        analyzer = SemanticAnalyzer(
            model_name="BAAI/bge-m3",
            use_keybert=True
        )

        response_embeddings = analyzer.analyze_ai_responses(ai_responses)
        print(f"âœ“ ì´ {len(response_embeddings)}ê°œì˜ ì„ë² ë”© ìƒì„± ì™„ë£Œ")

    step_times['ì„ë² ë”© ìƒì„±'] = time.time() - step_start
    print(f"â±ï¸  ì†Œìš” ì‹œê°„: {format_time(step_times['ì„ë² ë”© ìƒì„±'])}")

    # ì„ë² ë”© ì°¨ì› í™•ì¸
    first_embedding = list(response_embeddings.values())[0]['embedding']
    print(f"  ì„ë² ë”© ì°¨ì›: {first_embedding.shape[0]}")

    # ============================================================
    # STEP 4: BERTopic í´ëŸ¬ìŠ¤í„°ë§
    # ============================================================
    print("\n[STEP 4] BERTopicì„ ì‚¬ìš©í•œ í´ëŸ¬ìŠ¤í„°ë§")
    print("-" * 80)

    step_start = time.time()

    # ìºì‹œ í™•ì¸
    cached_result = None
    if use_cache:
        cached_result = cache_manager.load_clustered_responses()

    if cached_result is not None:
        clustered_responses, topic_keywords = cached_result
        print("âœ“ ìºì‹œì—ì„œ í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ ë¡œë“œ ì™„ë£Œ (ê³„ì‚° ê±´ë„ˆëœ€)")
    else:
        processor = EmbeddingProcessor(
            min_topic_size=3,  # ì‘ì€ ìƒ˜í”Œì´ë¯€ë¡œ ìµœì†Œ í¬ê¸° ì¤„ì„
            nr_topics=None,  # ìë™ ê²°ì •
            language="multilingual",
            verbose=True
        )

        # response_embeddingsì— content ì¶”ê°€ (BERTopicì´ ë¬¸ì„œ í…ìŠ¤íŠ¸ í•„ìš”)
        for response_id, emb_data in response_embeddings.items():
            # ai_responsesì—ì„œ í•´ë‹¹ response ì°¾ê¸°
            for resp in ai_responses:
                if resp.response_id == response_id:
                    emb_data['content'] = resp.content
                    break

        # BERTopic í´ëŸ¬ìŠ¤í„°ë§
        clustered_responses, topic_keywords = processor.cluster_with_bertopic(response_embeddings)

        print(f"\nâœ“ í´ëŸ¬ìŠ¤í„°ë§ ì™„ë£Œ")

        # ìºì‹œì— ì €ì¥
        if use_cache:
            cache_manager.save_clustered_responses(clustered_responses, topic_keywords)

    step_times['BERTopic í´ëŸ¬ìŠ¤í„°ë§'] = time.time() - step_start
    print(f"â±ï¸  ì†Œìš” ì‹œê°„: {format_time(step_times['BERTopic í´ëŸ¬ìŠ¤í„°ë§'])}")
    print(f"  ìƒì„±ëœ í† í”½ ìˆ˜: {len(set(cr.topic_id for cr in clustered_responses.values()))}")

    # ============================================================
    # STEP 5: ë¬¸ì„œë³„ í’€ë§
    # ============================================================
    print("\n[STEP 5] ëŒ€í™”ë³„ í† í”½ í’€ë§")
    print("-" * 80)

    step_start = time.time()

    # ìºì‹œ í™•ì¸
    document_embeddings = None
    if use_cache:
        document_embeddings = cache_manager.load_document_embeddings()

    if document_embeddings is None:
        # processorê°€ ì—†ìœ¼ë©´ ìƒì„± (ìºì‹œì—ì„œ clustered_responsesë¥¼ ë¡œë“œí•œ ê²½ìš°)
        if 'processor' not in locals():
            processor = EmbeddingProcessor(
                min_topic_size=3,
                nr_topics=None,
                language="multilingual",
                verbose=True
            )

        document_embeddings = processor.pool_by_conversation(
            clustered_responses,
            response_embeddings
        )

        print(f"âœ“ {len(document_embeddings)}ê°œì˜ ëŒ€í™”ì— ëŒ€í•œ ì£¼ì œ ë²¡í„° ìƒì„± ì™„ë£Œ")

        # ìºì‹œì— ì €ì¥
        if use_cache:
            cache_manager.save_document_embeddings(document_embeddings)
    else:
        print("âœ“ ìºì‹œì—ì„œ ë¬¸ì„œ ì„ë² ë”© ë¡œë“œ ì™„ë£Œ (ê³„ì‚° ê±´ë„ˆëœ€)")
        print(f"  {len(document_embeddings)}ê°œì˜ ëŒ€í™”")

    step_times['ë¬¸ì„œë³„ í’€ë§'] = time.time() - step_start
    print(f"â±ï¸  ì†Œìš” ì‹œê°„: {format_time(step_times['ë¬¸ì„œë³„ í’€ë§'])}")

    # ìƒ˜í”Œ ë¬¸ì„œ ì •ë³´ ì¶œë ¥
    if document_embeddings:
        print(f"\nìƒ˜í”Œ ë¬¸ì„œ:")
        sample_conv_id = list(document_embeddings.keys())[0]
        sample_doc = document_embeddings[sample_conv_id]
        print(f"  ëŒ€í™” ID: {sample_doc.conversation_id}")
        print(f"  ì œëª©: {sample_doc.conversation_title}")
        print(f"  ì£¼ì œ ìˆ˜: {len(sample_doc.topic_embeddings)}")
        print(f"  ì£¼ì œ ëª©ë¡:")
        for topic_id, keywords in sample_doc.topic_keywords.items():
            keywords_str = ", ".join(keywords[:5])
            print(f"    í† í”½ {topic_id}: {keywords_str}")

    # ============================================================
    # STEP 6: ìœ ì‚¬ë„ ê³„ì‚°
    # ============================================================
    print("\n[STEP 6] ë¬¸ì„œ ê°„ ìœ ì‚¬ë„ ê³„ì‚°")
    print("-" * 80)

    step_start = time.time()

    # ìºì‹œ í™•ì¸
    similarities = None
    if use_cache:
        similarities = cache_manager.load_similarities()

    if similarities is None:
        # processorê°€ ì—†ìœ¼ë©´ ìƒì„±
        if 'processor' not in locals():
            processor = EmbeddingProcessor(
                min_topic_size=3,
                nr_topics=None,
                language="multilingual",
                verbose=True
            )

        similarities = processor.compute_all_document_similarities(document_embeddings)

        print(f"\nâœ“ ìœ ì‚¬ë„ ê³„ì‚° ì™„ë£Œ")

        # ìºì‹œì— ì €ì¥
        if use_cache:
            cache_manager.save_similarities(similarities)
    else:
        print("âœ“ ìºì‹œì—ì„œ ìœ ì‚¬ë„ ë¡œë“œ ì™„ë£Œ (ê³„ì‚° ê±´ë„ˆëœ€)")

    step_times['ìœ ì‚¬ë„ ê³„ì‚°'] = time.time() - step_start
    print(f"â±ï¸  ì†Œìš” ì‹œê°„: {format_time(step_times['ìœ ì‚¬ë„ ê³„ì‚°'])}")

    # ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œ ìŒ 10ê°œ ì¶œë ¥
    print(f"\nê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œ ìŒ TOP 10:")
    sorted_sims = sorted(similarities.items(), key=lambda x: x[1], reverse=True)
    for i, ((conv_id_1, conv_id_2), sim) in enumerate(sorted_sims[:10], 1):
        doc1 = document_embeddings[conv_id_1]
        doc2 = document_embeddings[conv_id_2]
        print(f"  {i}. ìœ ì‚¬ë„ {sim:.4f}")
        print(f"     [{conv_id_1}] {doc1.conversation_title}")
        print(f"     [{conv_id_2}] {doc2.conversation_title}")

    # ============================================================
    # ì™„ë£Œ
    # ============================================================
    total_time = time.time() - total_start

    print("\n" + "=" * 80)
    print("ì „ì²´ ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print("=" * 80)

    print("\nğŸ“Š ë°ì´í„° ìš”ì•½:")
    print(f"  - ëŒ€í™” ìˆ˜: {len(conversations)}")
    print(f"  - AI ë‹µë³€ ìˆ˜: {len(ai_responses)}")
    print(f"  - ìƒì„±ëœ í† í”½ ìˆ˜: {len(topic_keywords)}")
    print(f"  - ë¬¸ì„œë³„ í‰ê·  ì£¼ì œ ìˆ˜: {sum(len(d.topic_embeddings) for d in document_embeddings.values()) / len(document_embeddings):.2f}")
    print(f"  - ìœ ì‚¬ë„ ìŒ ìˆ˜: {len(similarities)}")

    print("\nâ±ï¸  ì‹¤í–‰ ì‹œê°„ ìƒì„¸:")
    print("-" * 80)
    for step_name, step_time in step_times.items():
        percentage = (step_time / total_time) * 100
        print(f"  {step_name:20s} : {format_time(step_time):>10s}  ({percentage:5.1f}%)")
    print("-" * 80)
    print(f"  {'ì „ì²´ ì‹œê°„':20s} : {format_time(total_time):>10s}  (100.0%)")
    print("=" * 80)


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="ë¬¸ì„œ ì„ë² ë”© ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸")

    parser.add_argument(
        "--use-cache",
        action="store_true",
        help="ìºì‹œ ì‚¬ìš© (ì´ì „ ê³„ì‚° ê²°ê³¼ ì¬ì‚¬ìš©)"
    )

    parser.add_argument(
        "--clear-cache",
        action="store_true",
        help="ìºì‹œ ì´ˆê¸°í™” í›„ ì‹¤í–‰"
    )

    parser.add_argument(
        "--sample-size",
        type=int,
        default=50,
        help="ìƒ˜í”Œ ëŒ€í™” ê°œìˆ˜ (ê¸°ë³¸: 50)"
    )

    parser.add_argument(
        "--no-incremental",
        action="store_true",
        help="ì¦ë¶„ ìºì‹± ì‚¬ìš© ì•ˆ í•¨ (ì „ì²´ ì €ì¥/ë¡œë“œ ë°©ì‹)"
    )

    args = parser.parse_args()

    try:
        main(
            use_cache=args.use_cache,
            clear_cache=args.clear_cache,
            sample_size=args.sample_size,
            use_incremental=not args.no_incremental
        )
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
