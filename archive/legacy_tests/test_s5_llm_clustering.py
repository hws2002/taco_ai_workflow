"""
S5 ë‹¨ê³„ í…ŒìŠ¤íŠ¸: LLM ê¸°ë°˜ ëŒ€ë¶„ë¥˜ í´ëŸ¬ìŠ¤í„°ë§
(í‚¤ì›Œë“œ ê¸°ë°˜ìœ¼ë¡œ LLMì´ ì˜ë¯¸ë¡ ì  í´ëŸ¬ìŠ¤í„°ë¥¼ ìƒì„±)
"""

import sys
from pathlib import Path
import time
import json
import pickle

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from analyze.llm_clustering import LLMClusterer, extract_keywords_from_embeddings


def test_s5_llm_clustering(
    sample_size=50,
    use_cache=True,
    provider="openai",
    model="gpt-4",
    n_clusters=None
):
    """
    S5 í…ŒìŠ¤íŠ¸: LLM ê¸°ë°˜ ëŒ€ë¶„ë¥˜ í´ëŸ¬ìŠ¤í„°ë§

    ì›Œí¬í”Œë¡œìš°:
    1. S3 ê²°ê³¼(document_embeddings, response_embeddings) ë¡œë“œ
    2. í‚¤ì›Œë“œ ì¶”ì¶œ
    3. LLMì—ê²Œ í´ëŸ¬ìŠ¤í„°ë§ ìš”ì²­
    4. ê²°ê³¼ ì €ì¥
    """
    start_time = time.time()
    times = {}

    print("=" * 80)
    print("S5 ë‹¨ê³„ í…ŒìŠ¤íŠ¸: LLM ê¸°ë°˜ ëŒ€ë¶„ë¥˜ í´ëŸ¬ìŠ¤í„°ë§")
    print("=" * 80)

    output_dir = project_root / "test" / "output"
    output_dir.mkdir(exist_ok=True)

    # ============================================================
    # 1. S3 ê²°ê³¼ ë¡œë“œ
    # ============================================================
    print("\n[1] S3 ê²°ê³¼ ë¡œë“œ")
    print("-" * 80)

    load_start = time.time()

    # Document embeddings ë¡œë“œ
    doc_embeddings_file = output_dir / "s3_document_embeddings.pkl"
    if not doc_embeddings_file.exists():
        print(f"âŒ S3 ê²°ê³¼ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {doc_embeddings_file}")
        print("ë¨¼ì € test_s3_new.pyë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
        return None

    with open(doc_embeddings_file, 'rb') as f:
        document_embeddings = pickle.load(f)

    print(f"âœ“ {len(document_embeddings)}ê°œì˜ ë¬¸ì„œ ì„ë² ë”© ë¡œë“œ")

    # Response embeddings ë¡œë“œ
    embeddings_file = output_dir / "s2_embeddings.pkl"
    if not embeddings_file.exists():
        print(f"âŒ S2 ê²°ê³¼ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {embeddings_file}")
        print("ë¨¼ì € test_s2_new.pyë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
        return None

    with open(embeddings_file, 'rb') as f:
        response_embeddings = pickle.load(f)

    print(f"âœ“ {len(response_embeddings)}ê°œì˜ ì‘ë‹µ ì„ë² ë”© ë¡œë“œ")

    times['loading'] = time.time() - load_start
    print(f"â±ï¸  ì†Œìš” ì‹œê°„: {times['loading']:.2f}ì´ˆ")

    # ============================================================
    # 2. í‚¤ì›Œë“œ ì¶”ì¶œ
    # ============================================================
    print("\n" + "=" * 80)
    print("[2] í‚¤ì›Œë“œ ì¶”ì¶œ")
    print("-" * 80)

    extract_start = time.time()

    # document_embeddingsì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
    chats_data = extract_keywords_from_embeddings(
        document_embeddings=document_embeddings,
        response_embeddings=response_embeddings,
        top_k=10  # ê° ì±„íŒ…ë‹¹ ìƒìœ„ 10ê°œ í‚¤ì›Œë“œ
    )

    print(f"âœ“ {len(chats_data['chats'])}ê°œ ì±„íŒ…ì˜ í‚¤ì›Œë“œ ì¶”ì¶œ ì™„ë£Œ")

    # ìƒ˜í”Œ ì¶œë ¥
    if chats_data['chats']:
        print(f"\nìƒ˜í”Œ (ì²˜ìŒ 3ê°œ ì±„íŒ…):")
        for i, chat in enumerate(chats_data['chats'][:3], 1):
            print(f"\n{i}. {chat['chat_id']} - {chat.get('title', 'N/A')}")
            keywords_str = ", ".join([f"{kw['word']}({kw['score']:.2f})"
                                     for kw in chat['keywords'][:5]])
            print(f"   í‚¤ì›Œë“œ: {keywords_str}")

    times['extraction'] = time.time() - extract_start
    print(f"\nâ±ï¸  ì†Œìš” ì‹œê°„: {times['extraction']:.2f}ì´ˆ")

    # í‚¤ì›Œë“œ ë°ì´í„° ì €ì¥ (ì¤‘ê°„ ê²°ê³¼)
    keywords_file = output_dir / "s5_keywords.json"
    with open(keywords_file, 'w', encoding='utf-8') as f:
        json.dump(chats_data, f, ensure_ascii=False, indent=2)
    print(f"âœ“ í‚¤ì›Œë“œ ë°ì´í„° ì €ì¥: {keywords_file}")

    # ============================================================
    # 3. LLM í´ëŸ¬ìŠ¤í„°ë§
    # ============================================================
    print("\n" + "=" * 80)
    print("[3] LLM í´ëŸ¬ìŠ¤í„°ë§")
    print("-" * 80)

    clustering_start = time.time()

    try:
        # LLM í´ëŸ¬ìŠ¤í„°ëŸ¬ ì´ˆê¸°í™”
        clusterer = LLMClusterer(
            provider=provider,
            model=model
        )

        # í´ëŸ¬ìŠ¤í„°ë§ ì‹¤í–‰
        result = clusterer.cluster_chats(
            chats_data=chats_data,
            n_clusters=n_clusters,
            temperature=0.3  # ë‚®ì€ ì˜¨ë„ë¡œ ì¼ê´€ì„± í™•ë³´
        )

        times['clustering'] = time.time() - clustering_start
        print(f"\nâ±ï¸  ì†Œìš” ì‹œê°„: {times['clustering']:.2f}ì´ˆ")

    except Exception as e:
        print(f"âŒ LLM í´ëŸ¬ìŠ¤í„°ë§ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return None

    # ============================================================
    # 4. ê²°ê³¼ ì €ì¥
    # ============================================================
    print("\n" + "=" * 80)
    print("[4] ê²°ê³¼ ì €ì¥")
    print("-" * 80)

    save_start = time.time()

    # í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ ì €ì¥
    result_file = output_dir / "s5_llm_clustering_result.json"
    with open(result_file, 'w', encoding='utf-8') as f:
        json.dump(result, f, ensure_ascii=False, indent=2)

    print(f"âœ“ í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ ì €ì¥: {result_file}")
    print(f"  - íŒŒì¼ í¬ê¸°: {result_file.stat().st_size / 1024:.2f} KB")

    # í†µê³„ ì •ë³´ ì €ì¥
    times['saving'] = time.time() - save_start
    times['total'] = time.time() - start_time

    # í´ëŸ¬ìŠ¤í„° ë¶„í¬ ê³„ì‚°
    from collections import Counter
    cluster_counts = Counter(assignment['cluster_id']
                            for assignment in result['chat_assignments'])

    stats = {
        "total_chats": len(chats_data['chats']),
        "total_clusters": len(result.get('cluster_definitions', {})),
        "cluster_distribution": dict(cluster_counts),
        "llm_provider": provider,
        "llm_model": model,
        "elapsed_time": {
            "loading_seconds": round(times['loading'], 2),
            "extraction_seconds": round(times['extraction'], 2),
            "clustering_seconds": round(times['clustering'], 2),
            "saving_seconds": round(times['saving'], 2),
            "total_seconds": round(times['total'], 2)
        }
    }

    stats_file = output_dir / "s5_stats.json"
    with open(stats_file, 'w', encoding='utf-8') as f:
        json.dump(stats, f, ensure_ascii=False, indent=2)

    print(f"âœ“ í†µê³„ ì €ì¥: {stats_file}")

    # ============================================================
    # ê²°ê³¼ ìš”ì•½
    # ============================================================
    print("\n" + "=" * 80)
    print("S5 ë‹¨ê³„ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print("=" * 80)

    print(f"\nâ±ï¸  ì†Œìš” ì‹œê°„:")
    print(f"  - ë°ì´í„° ë¡œë“œ: {times['loading']:.2f}ì´ˆ")
    print(f"  - í‚¤ì›Œë“œ ì¶”ì¶œ: {times['extraction']:.2f}ì´ˆ")
    print(f"  - LLM í´ëŸ¬ìŠ¤í„°ë§: {times['clustering']:.2f}ì´ˆ")
    print(f"  - ê²°ê³¼ ì €ì¥: {times['saving']:.2f}ì´ˆ")
    print(f"  - ì „ì²´: {times['total']:.2f}ì´ˆ")

    print(f"\nğŸ“Š ê²°ê³¼ ìš”ì•½:")
    print(f"  - ì±„íŒ… ìˆ˜: {len(chats_data['chats'])}ê°œ")
    print(f"  - í´ëŸ¬ìŠ¤í„° ìˆ˜: {len(result.get('cluster_definitions', {}))}ê°œ")
    print(f"  - LLM: {provider}/{model}")

    print(f"\nğŸ“ ì¶œë ¥ íŒŒì¼:")
    print(f"  - {keywords_file}")
    print(f"  - {result_file}")
    print(f"  - {stats_file}")

    return result


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="S5 í…ŒìŠ¤íŠ¸: LLM ê¸°ë°˜ ëŒ€ë¶„ë¥˜ í´ëŸ¬ìŠ¤í„°ë§")
    parser.add_argument("--sample-size", type=int, default=50, help="ìƒ˜í”Œ ëŒ€í™” ê°œìˆ˜")
    parser.add_argument("--provider", type=str, default="openai",
                       choices=["openai", "anthropic"], help="LLM ì œê³µì")
    parser.add_argument("--model", type=str, default="gpt-4",
                       help="LLM ëª¨ë¸ (ì˜ˆ: gpt-4, gpt-3.5-turbo, claude-3-opus-20240229)")
    parser.add_argument("--n-clusters", type=int, default=None,
                       help="í´ëŸ¬ìŠ¤í„° ê°œìˆ˜ (ë¯¸ì§€ì •ì‹œ LLMì´ ìë™ ê²°ì •)")
    parser.add_argument("--no-cache", action="store_true", help="ìºì‹œ ì‚¬ìš© ì•ˆ í•¨")

    args = parser.parse_args()

    try:
        result = test_s5_llm_clustering(
            sample_size=args.sample_size,
            use_cache=not args.no_cache,
            provider=args.provider,
            model=args.model,
            n_clusters=args.n_clusters
        )

        if result:
            print(f"\nâœ… S5 í…ŒìŠ¤íŠ¸ ì„±ê³µ!")

    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
